---
title: "Data Science II Homework 1"
author: "Camille Okonkwo"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '2'
header-includes:
- \usepackage{fancyhdr}
- \usepackage{lipsum}
- \pagestyle{fancy}
- \fancyhead[R]{\thepage}
- \fancypagestyle{plain}{\pagestyle{fancy}}
editor_options: 
  chunk_output_type: console
--- 
\newpage

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = TRUE, 
  warning = FALSE,
  fig.width = 6, 
  fig.asp = .6, 
  out.width = "90%"
  )
```

```{r, echo = T, message = FALSE, results='hide', warning=FALSE}
library(caret)
library(tidymodels)
library(ggplot2)
```

### loading training and testing data 
```{r}
testing_data = read_csv("data/housing_test.csv")

training_data = read_csv("data/housing_training.csv")
```

### specifying predictors and response variables
```{r}
# training data
x <- model.matrix(Sale_Price ~ ., training_data)[, -1]
y <- training_data$Sale_Price

# testing data
x2 <- model.matrix(Sale_Price ~ .,testing_data)[, -1]
y2 <- testing_data$Sale_Price
```


# 1a) Fit a lasso model on the training data. Report the selected tuning parameter and the test error. When the 1SE rule is applied, how many predictors are included in the model?

### fitting lasso model on the training data using `caret`
```{r lasso_fit}
# creating a training control with a 10-fold cross-validation
ctrl1 <- trainControl(method = "cv", 
                      number = 10, 
                      selectionFunction = "best")

set.seed(2)

# fitting lasso model
lasso.fit <- train(Sale_Price ~.,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(6, 0, length = 100))),
                   trControl = ctrl1
)

plot(lasso.fit, xTrans = log)

# tuning parameter
lasso.fit$bestTune

# prediction
lasso.pred <- predict(lasso.fit, newdata = testing_data)

# test error
mean((lasso.pred - testing_data$Sale_Price)^2)
```
The best tuning parameter selected for the lasso model using the 1SE rule is a lambda = `r lasso.fit$bestTune$lambda`. The test error is `r mean((lasso.pred - testing_data$Sale_Price)^2)`.

### applying 1SE
```{r lasso_1se}
# creating a training control with a 10-fold cross-validation using 1se criteria
ctrl2 <- trainControl(method = "cv", 
                      number = 10, 
                      selectionFunction = "oneSE")

set.seed(2)

# 1SE lasso
lasso.1se <- train(Sale_Price ~.,
                   data = training_data,
                   method = "glmnet",
                   tuneGrid = expand.grid(alpha = 1,
                                          lambda = exp(seq(6, 0, length = 100))),
                   trControl = ctrl2
)

plot(lasso.1se, xTrans = log)

# extracting coefficients in the 1se model
coef(lasso.1se$finalModel, lasso.1se$bestTune$lambda)
```
When the 1SE rule is applied, there are 36 predictors included in the model. 

\newpage

# 1b) Fit an elastic net model on the training data. Report the selected tuning parameters and the test error. Is it possible to apply the 1SE rule to select the tuning parameters for elastic net? If the 1SE rule is applicable, implement it to select the tuning parameters. If not, explain why. 


### fitting an elastic net model on the training data using `caret`
```{r enet_fit}
set.seed(2)

# elastic net model
enet.fit <- train(Sale_Price ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 25),
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl1)
# CV plot
myCol <- rainbow(25)
myPar <- list(superpose.symbol = list(col = myCol),
              superpose.line = list(col = myCol)) 

plot(enet.fit, par.settings = myPar)

# tuning parameter
enet.fit$bestTune

# prediction
enet.pred <- predict(enet.fit, newdata = testing_data)

# test error
mean((enet.pred - testing_data$Sale_Price)^2)
```
The best tuning parameter selected for the elastic net model is an alpha = `r enet.fit$bestTune$alpha` and a lambda = `r 37`. The test error is `r mean((enet.pred - testing_data$Sale_Price)^2)`.

### applying 1SE rule to elastic net model in `caret`
```{r enet_1se}
set.seed(2)

# 1SE cross validation
ctrl2 <- trainControl(method = "cv",
                      number = 10,
                      selectionFunction = "oneSE")
# elastic net model
enet.fit.1se <- train(Sale_Price ~ .,
                  data = training_data,
                  method = "glmnet",
                  tuneGrid = expand.grid(alpha = seq(0, 1, length = 25),
                                         lambda = exp(seq(10, -5, length = 100))),
                  trControl = ctrl2)

# tuning parameter
enet.fit.1se$bestTune

# prediction
enet.pred.1se <- predict(enet.fit.1se, newdata = testing_data)

# test error
mean((enet.pred.1se - testing_data$Sale_Price)^2)
```
The best tuning parameter selected for the elastic net model using 1SE is an alpha = `r enet.fit.1se$bestTune$alpha` and a lambda = `r enet.fit.1se$bestTune$lambda`. The test error is `r mean((enet.pred.1se - testing_data$Sale_Price)^2)`. An alpha value of zero signifies pure ridge regression, and instead of the mix of L1 and L2 regularization we see in elastic net, it becomes only L2 and the penalty term for the L1 normalization is removed from the optimization objective. That being said, 1SE is not applicable in this case to elastic net. 

\newpage

# 1c) Fit a partial least squares model on the training data and report the test error. How many components are included in your model?

### PLS using `caret`
```{r pls_fit}
set.seed(2)

# model look up
modelLookup("pls")

# partial least squares model
pls.fit <- train(x, 
                 y,
                 method = "pls",
                 tuneGrid = data.frame(ncomp = 1:39),
                 trControl = ctrl2,
                 preProcess = c("center", "scale"))

# predict
predy2.pls2 <- predict(pls.fit, newdata = x2)

# components
pls.fit$bestTune$ncomp

# test error
mean((y2 - predy2.pls2)^2)

# components plot
ggplot(pls.fit, highlight = TRUE) + theme_bw()
```
From the components plot, we see there are `r pls.fit$bestTune$ncomp` components chosen for this model. The test error is `r mean((y2 - predy2.pls2)^2)`. 

\newpage
# 1d) Choose the best model for predicting the response and explain your choice.

### comparing models
```{r model_compare}
set.seed(2)

resamp <- resamples(list(enet = enet.fit, lasso = lasso.fit, pls = pls.fit))

summary(resamp)

parallelplot(resamp, metric = "RMSE")

bwplot(resamp, metric = "RMSE")
```
From the resampling summary, I believe the best model is the elastic net since it has the smallest mean RMSE value. If you wish to use the median RMSE value, the best model is the lasso since it has the smallest median RMSE.

\newpage

# 1e) If “caret” was used for the elastic net in (b), retrain this model with “tidymodels”, and vice versa. Compare the selected tuning parameters between the two software approaches. Should there be discrepancies in the chosen parameters, discuss potential reasons for these differences.

### retraining elastic net with `tidymodels`
```{r enet_tidy}
set.seed(2)

cv_folds <- vfold_cv(training_data, v = 10)

# model specification for elastic net
enet_spec <- linear_reg(penalty = tune(), mixture = tune()) |>
  set_engine("glmnet") |>
  set_mode("regression")

# grid of tuning parameters
enet_grid_set <- parameters(penalty(range = c(3, 6),
                                    trans = log_trans()),
                            mixture(range = c(0, 1)))

enet_grid <- grid_regular(enet_grid_set, levels = c(100, 25))

# set up workflow
enet_workflow <- workflow() |>
  add_model(enet_spec) |>
  add_formula(Sale_Price ~ .)

set.seed(2)

# tuning model
enet_tune <- tune_grid(
  enet_workflow,
  resamples = cv_folds,
  grid = enet_grid
)

# CV plot
autoplot(enet_tune, metric = "rmse") + 
  theme(legend.position = "top") +
  labs(color = "Mixing Percentage\n(Alpha Values)") 

# selecting best tuning parameters
enet_best <- select_best(enet_tune, metric = "rmse")

print(enet_best)

enet_best$mixture

# update model with the best lambda
final_enet_spec <- enet_spec |> 
  update(penalty = enet_best$penalty,
         mixture = enet_best$mixture)

# fit model to the train data
enet_fit <- fit(final_enet_spec,
                formula = Sale_Price ~ .,
                data = training_data)

# extract coefficients
enet_model <- extract_fit_engine(enet_fit)

coef(enet_model, s = enet_best$penalty)

# prediction
enet_pred <- predict(enet_fit, new_data = testing_data)

# test RMSE
sqrt(mean((enet_pred[[1]] - testing_data$Sale_Price)^2))

37
```
Using `tidymodels` the alpha `r enet_best$mixture` and the penalty (lambda) is `r enet_best$penalty`. Using caret, the alpha is `r enet.fit$bestTune$alpha` and the lambda is `r enet.fit$bestTune$lambda`. These are likely different because different packages use different compilation methods, so they do not yield universally identical results. 